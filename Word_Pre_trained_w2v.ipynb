{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_G_Pre-trained_w2v.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VxUikuZXniP4",
        "d8CsQsc0nyau",
        "yKR8J4rxd-N0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/habibsifat/BengaliDictionary/blob/master/Word_Pre_trained_w2v.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v1Yz4K5Daml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AvyzSKmBBPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "__author__ = 'maxim'\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import string\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.utils.data_utils import get_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SH2kk0FB3Sq",
        "colab_type": "code",
        "outputId": "034e4fea-9c6b-4847-adbe-7b25822b7379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "!ls 'drive/My Drive/Colab Notebooks/Capstone/Lenth-5.txt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access 'drive/My Drive/Colab Notebooks/Capstone/Lenth-5.txt': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzeYYfHrDx-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_file = 'drive/My Drive/Colab Notebooks/Capstone/Lenth-5.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFrgy4a4B4o8",
        "colab_type": "code",
        "outputId": "ced5a088-2752-4a54-e40f-e3787603bd82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "print('\\nPreparing the sentences...')\n",
        "#max_sentence_len = 40\n",
        "\n",
        "sentences = []\n",
        "with open(corpus_file, 'r', encoding=\"utf-8-sig\") as f:\n",
        "    for line in f:\n",
        "        sentences.append(line.split())\n",
        "        \n",
        "print('Num sentences:', len(sentences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing the sentences...\n",
            "Num sentences: 4626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxUikuZXniP4",
        "colab_type": "text"
      },
      "source": [
        "# W2V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh5wTIw0CnLh",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#Training word2vec \n",
        "\n",
        "print('\\nTraining word2vec...')\n",
        "word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, window=5, iter=5)\n",
        "pretrained_weights = word_model.wv.syn0\n",
        "vocab_size, emdedding_size = pretrained_weights.shape\n",
        "print('Result embedding shape:', pretrained_weights.shape)\n",
        "#print('Checking similar words:')\n",
        "#for word in ['৬ষ্ঠ', 'ঘুরে', 'সময়ই', 'করানো']:\n",
        " # most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.most_similar(word)[:8])\n",
        "#print('  %s -> %s' % (word, most_similar))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhcCAm__NEmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_model.wv.vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs3FsKUcD34-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2idx(word):\n",
        "  return word_model.wv.vocab[word].index\n",
        "def idx2word(idx):\n",
        "  return word_model.wv.index2word[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2wjLTJCpPII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = \"কার্তিক\"  # for any word in model\n",
        "i = word_model.wv.vocab[word].index\n",
        "print(i)\n",
        "print(word_model.wv.index2word[18])\n",
        "if word_model.wv.index2word[i] == word :\n",
        "  # will be true\n",
        "  print(\"True\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4M2QfvFGJiX",
        "colab_type": "code",
        "outputId": "8881b5a0-f4d2-4e4c-8b78-9ef1a5a2450b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#print(word_model.wv.vocab[\"ও\"].index)\n",
        "print(word_model.wv.index2word[869])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ইউনিয়ন\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8qKp_8JnozK",
        "colab_type": "text"
      },
      "source": [
        "# Data Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBGzpNss16Z9",
        "colab_type": "code",
        "outputId": "bf75b2f5-248d-449d-ff8c-55fcb6cfbd02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "print('\\nPreparing the sentences...')\n",
        "corpus_file = 'drive/My Drive/Colab Notebooks/Capstone/Lenth-5.txt'\n",
        "max_sentence_len = 5\n",
        "\n",
        "sentences = []\n",
        "with open(corpus_file, 'r', encoding=\"utf-8-sig\") as f:\n",
        "    for line in f:\n",
        "        sentences.append(line.split())\n",
        "        \n",
        "print('Num sentences:', len(sentences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing the sentences...\n",
            "Num sentences: 4626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76wHBCCOEcFR",
        "colab_type": "code",
        "outputId": "8dd399c8-a4f3-4abc-e1ec-0d5bbac0fb10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "print('\\nPreparing the data for LSTM...')\n",
        "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
        "train_y = np.zeros([len(sentences)], dtype=np.int32)\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, word in enumerate(sentence[:-1]):\n",
        "    train_x[i, t] = word2idx(word)\n",
        "  train_y[i] = word2idx(sentence[-1])\n",
        "print('train_x shape:', train_x.shape)\n",
        "print('train_y shape:', train_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing the data for LSTM...\n",
            "train_x shape: (4626, 5)\n",
            "train_y shape: (4626,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8CsQsc0nyau",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKrIxmdwEkwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('\\nTraining LSTM...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
        "model.add(LSTM(units=emdedding_size,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=emdedding_size, return_sequences=False))\n",
        "model.add(Dense(units=vocab_size))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKR8J4rxd-N0",
        "colab_type": "text"
      },
      "source": [
        "# Model Predict\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuRh-xUBFJss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def sample(preds, temperature=1.0):\n",
        "  if temperature <= 0:\n",
        "    #return np.argmax(preds)\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  print(\"Preds : \",preds)\n",
        "  #probas = np.random.multinomial(1, preds, 1)\n",
        "  #return np.argmax(probas)\n",
        "  #return np.argmax(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgSvLvttFPZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_next(text, num_generated=1):\n",
        "  word_idxs = [word2idx(word) for word in text.split()]\n",
        "  print(word_idxs)\n",
        "  for i in range(num_generated):\n",
        "    prediction = model.predict(x=np.array(word_idxs)) #print dimension\n",
        "    print(\"prediction:\",end='')\n",
        "    print(prediction[-1])\n",
        "    print(np.shape(prediction))\n",
        "    maxx=np.amax(prediction[-1])\n",
        "    print(\"Max : \",maxx)\n",
        "    print(\"Sum \",np.sum(prediction[-1]))\n",
        "    #idx = sample(prediction[-1], temperature=0.7)\n",
        "    j=0\n",
        "    for a in prediction[-1]:\n",
        "      #print(\"Value of \",end='')\n",
        "      #print(j,end='')\n",
        "      if a==maxx:\n",
        "        print(\"Max : \",a)\n",
        "        idx=j\n",
        "      j=j+1\n",
        "      #print()\n",
        "    #idx=np.argmax(prediction[-1])\n",
        "    print(\"idx:\",end='')\n",
        "    print(idx)\n",
        "    word_idxs.append(idx)\n",
        "    print(word_idxs)\n",
        "  return ' '.join(idx2word(idx) for idx in word_idxs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJLUJZuSFY4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "  print('\\n For Lenth-5 Generating text after epoch: %d' % epoch)\n",
        "  texts = [\n",
        "    '* তড়িৎ প্রকৌশল বলতে ',\n",
        "      '* এটি বাংলাদেশে বাংলা '\n",
        "  ]\n",
        "  for text in texts:\n",
        "    sample = generate_next(text)\n",
        "    print('%s -> %s' % (text, sample))\n",
        "\n",
        "model.fit(train_x, train_y,\n",
        "          batch_size=128,\n",
        "          epochs=1,\n",
        "callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ2VsNzq4Ww8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Partly train model\n",
        "#model.fit(first_training, first_classes, batch_size=32, nb_epoch=20)\n",
        "\n",
        "#Save partly trained model\n",
        "model.save('word_generate.h5')\n",
        "\n",
        "#Load partly trained model\n",
        "from keras.models import load_model\n",
        "model = load_model('word_generate.h5')\n",
        "\n",
        "#Continue training\n",
        "#model.fit(second_training, second_classes, batch_size=32, nb_epoch=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzNXTFL9BG-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_next(text, num_generated=1):\n",
        "  word_idxs = [word2idx(word) for word in text.split()]\n",
        "  #print(word_idxs)\n",
        "  Str1 = \"অনেক\"\n",
        "  for i in range(num_generated):\n",
        "    prediction = model.predict(x=np.array(word_idxs)) #print dimension\n",
        "    max_word = prediction[-1].argsort()[-10:][::-1]\n",
        "    #print(max_word)\n",
        "    j=1\n",
        "    for idx in max_word:\n",
        "      word_idxs.append(idx)\n",
        "      Distance = levenshtein_ratio_and_distance(Str,idx)\n",
        "      Ratio = levenshtein_ratio_and_distance(Str1,Str2,ratio_calc = True)\n",
        "      print(\"Max word %d : %s \" % (j,idx2word(idx)))\n",
        "      print(\"Distance : %s And Ratio : %s \" % (Distance,Ratio)\n",
        "        #j=j+1\n",
        "      #print(word_idxs)\n",
        "return ' '.join(idx2word(idx) for idx in word_idxs)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}